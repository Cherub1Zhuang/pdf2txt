Image Transformer Niki Parmar Ashish Vaswani Jakob Uszkoreit ukasz Kaiser Noam Shazeer Alexander Ku Dustin Tran arXiv cs CV Jun Abstract Image generation has been successfully cast as an autoregressive sequence generation or trans formation problem Recent work has shown that self attention is an effective way of modeling tex tual sequences In this work we generalize recently proposed model architecture based on self attention the Transformer to sequence modeling formulation of image generation with tractable likelihood By restricting the self attention mechanism to attend to local neighbor Table Three outputs of CelebA super resolution model fol hoods we significantly increase the size of im ages the model can process in practice despite lowed by three image completions by conditional CIFAR maintaining significantly larger receptive fields model with input model output and the original from left to right per layer than typical convolutional neural net tractable likelihood Beyond licensing the comparatively works While conceptually simple our generative models significantly outperform the current state of the art in image generation on ImageNet im simple and stable training regime of directly maximizing proving the best published negative log likelihood log likelihood this enables the straightforward application on ImageNet from to We also present of these models in problems such as image compression results on image super resolution with large van den Oord Schrauwen and probabilistic plan magnification ratio applying an encoder decoder ning and exploration Bellemare et al configuration of our architecture In human eval The likelihood is made tractable by modeling the joint dis uation study we find that images generated by tribution of the pixels in the image as the product of condi our super resolution model fool human observers tional distributions Larochelle Murray Theis three times more often than the previous state of Bethge Thus turning the problem into sequence the art modeling problem the state of the art approaches apply Introduction recurrent or convolutional neural networks to predict each next pixel given all previously generated pixels van den Recent advances in modeling the distribution of natural Oord et al Training recurrent neural networks to sequentially predict each pixel of even small image images with neural networks allow them to generate increas is computationally very challenging Thus parallelizable ingly natural looking images Some models such as the models that use convolutional neural networks such as the PixelRNN and PixelCNN van den Oord et al have PixelCNN have recently received much more attention and Equal contribution Ordered by coin flip Google Brain Mountain View USA Department of Electrical Engineering and Computer Sciences University of California Berkeley Work done during an internship at Google Brain Google AI Mountain have now surpassed the PixelRNN in quality van den Oord et al One disadvantage of CNNs compared to RNNs is their View USA Correspondence to Ashish Vaswani Niki Parmar typically fairly limited receptive field This can adversely Jakob Uszkoreit avaswani google com nikip google com affect their ability to model long range phenomena common usz google com in images such as symmetry and occlusion especially with Proceedings of the thInternational Conference on Machine small number of layers Growing the receptive field has been shown to improve quality significantly Salimans et al Learning Stockholm Sweden PMLR Copyright by the author Doing so however comes at significant cost in number Image Transformer of parameters and consequently computational performance and can make training such models more challenging In this work we show that self attention Cheng et al Parikh et al Vaswani et al can achieve better balance in the trade off between the virtually unlimited receptive field of the necessarily sequential PixelRNN and the limited receptive field of the much more parallelizable PixelCNN and its various extensions We adopt similar factorizations of the joint pixel distribu tion as previous work Following recent work on model ing text Vaswani et al however we propose es chewing recurrent and convolutional networks in favor of the Image Transformer model based entirely on self attention mechanism The specific locally restricted form of multi head self attention we propose can be interpreted as sparsely parameterized form of gated convolution By decoupling the size of the receptive field from the num ber of parameters this allows us to use significantly larger receptive fields than the PixelCNN Despite comparatively low resource requirements for train ing the Image Transformer attains new state of the art in modeling images from the standard ImageNet data set as measured by log likelihood Our experiments indicate that increasing the size of the receptive field plays sig nificant role in this improvement We observe significant improvements up to effective receptive field sizes of pixels while the PixelCNN van den Oord et al with filters used Many applications of image density models require condi tioning on additional information of various kinds from im ages in enhancement or reconstruction tasks such as super resolution in painting and denoising to text when synthesiz ing images from natural language descriptions Mansimov et al In visual planning tasks conditional image generation models could predict future frames of video con ditioned on previous frames and taken actions In this work we hence also evaluate two different methods of performing conditional image generation with the Im age Transformer In image class conditional generation we condition on an embedding of one of small number of image classes In super resolution with high magnification ratio we condition on very low resolution image employing the Image Transformer in an encoder decoder configuration Kalchbrenner Blunsom In com parison to recent work on autoregressive super resolution Dahl et al human evaluation study found im ages generated by our models to look convincingly natural significantly more often Background There is broad variety of types of image generation mod els in the literature This work is strongly inspired by au toregressive models such as fully visible belief networks and NADE Bengio Bengio Larochelle Mur ray in that we also factor the joint probability of the image pixels into conditional distributions Following PixelRNN van den Oord et al we also model the color channels of the output pixels as discrete values gener ated from multinomial distribution implemented using simple softmax layer The current state of the art in modeling images on CIFAR data set was achieved by PixelCNN which models the output pixel distribution with discretized logistic mixture likelihood conditioning on whole pixels instead of color channels and changes to the architecture Salimans et al These modifications are readily applicable to our model which we plan to evaluate in future work Another popular direction of research in image generation is training models with an adversarial loss Goodfellow et al Typically in this regime generator net work is trained in opposition to discriminator network trying to determine if given image is real or generated In contrast to the often blurry images generated by networks trained with likelihood based losses generative adversar ial networks GANs have been shown to produce sharper images with realistic high frequency detail in generation and image super resolution tasks Zhang et al Ledig et al While very promising GANs have various drawbacks They are notoriously unstable Radford et al motivating large number of methods attempting to make their train ing more robust Metz et al Berthelot et al Another common issue is that of mode collapse where gen erated images fail to reflect the diversity in the training set Metz et al related problem is that GANs do not have density in closed form This makes it challenging to measure the degree to which the models capture diversity This also complicates model design Objectively evaluating and com paring say different hyperparameter choices is typically much more difficult in GANs than in models with tractable likelihood Model Architecture Image Representation We treat pixel intensities as either discrete categories or ordi nal values this setting depends on the distribution Section For categories each of the input pixels three color channels is encoded using channel specific set of Image Transformer Input Gen Truth Input Gen Truth Table On the left are image completions from our best conditional generation model where we sample the second half On the right are samples from our four fold super resolution model trained on CIFAR Our images look realistic and plausible show good diversity among the completion samples and observe the outputs carry surprising details for coarse inputs in super resolution dimensional embedding vectors of the intensity values Local Self Attention Dropout cmp MatMul Wv MatMul Wq MatMul Wk For output intensities we share single separate set of dimensional embeddings across the channels LayerNorm For an image of width and height we combine the LayerNorm width and channel dimensions yielding dimensional tensor with shape FFNN For ordinal values we run window size strided MatMul Wv Dropout convolution to combine the channels per pixel to form an input representation with shape To each pixel representation we add dimensional en MatMul Wv coding of coordinates of that pixel We evaluated two dif ferent coordinate encodings sine and cosine functions of softmax the coordinates with different frequencies across different cmp cmp dimensions following Vaswani et al and learned position embeddings Since we need to represent two co ordinates we use of the dimensions to encode the row pq number and the other of the dimensions to encode the the column and color channel Self Attention For image conditioned generation as in our super resolution models we use an encoder decoder architecture The en coder generates contextualized per pixel channel repre sentation of the source image The decoder autoregressively generates an output image of pixel intensities one channel per pixel at each time step While doing so it consumes the previously generated pixels and the input image represen Figure slice of one layer of the Image Transformer recom puting the representation of single channel of one pixel by attending to memory of previously generated pixels After performing local self attention we apply two layer position wise feed forward neural network with the same parameters for all positions in given layer Self attention and the feed forward networks are followed by dropout and bypassed by residual connection with subsequent layer normalization The position encodings pq are added only in the first layer Image Transformer tation generated by the encoder For both the encoder and decoder the Image Transformer uses stacks of self attention and position wise feed forward layers similar to Vaswani et al In addition the decoder uses an attention mechanism to consume the encoder representation For un conditional and class conditional generation we employ the Image Transformer in decoder only configuration Before we describe how we scale self attention to images comprised of many more positions than typically found in sentences we give brief description of self attention Each self attention layer computes dimensional repre sentation for each position that is each channel of each pixel To recompute the representation for given posi tion it first compares the position current representation to other positions representations obtaining an attention distribution over the other positions This distribution is then used to weight the contribution of the other positions representations to the next representation for the position at hand Equations and outline the computation in our self attention and fully connected feed forward layers Figure depicts it and are the parameters of the feed forward layer and are shared across all the positions in layer These fully describe all operations performed in every layer independently for each position with the exception of multi head attention For details of multi head self attention see Vaswani et al qa layernorm dropout softmax Wqq MWk MWv layernorm qa dropout ReLu qa In more detail following previous work we call the cur rent representation of the pixel channel or position to be recomputed the query The other positions whose repre sentations will be used in computing new representation for are which together comprise the columns of the memory matrix Note that can also contain We first transform and linearly by learned matrices Wq and Wk respectively The self attention mechanism then compares to each of the pixel channel representations in the memory with dot product scaled by We apply the softmax function to the resulting compatibility scores treating the obtained vector as attention distribution over the pixel channels in the memory After applying another linear transformation Wv to the memory we compute weighted average of the transformed memory weighted by the attention distribu tion In the decoders of our different models we mask the outputs of the comparisons appropriately so that the model cannot attend to positions in the memory that have not been generated yet To the resulting vector we then apply single layer fully connected feed forward neural network with rectified linear activation followed by another linear transformation The learned parameters of these are shared across all positions but different from layer to layer As illustrated in Figure we perform dropout merge in residual connections and perform layer normalization after each application of self attention and the position wise feed forward networks Ba et al Srivastava et al The entire self attention operation can be implemented using highly optimized matrix multiplication code and executed in parallel for all pixels channels Local Self Attention The number of positions included in the memory lm or the number of columns of has tremendous impact on the scalability of the self attention mechanism which has time complexity in lm The encoders of our super resolution models operate on pixel images and it is computationally feasible to attend to all of their positions The decoders in our experiments rendering attending to all positions impractical however produce pixel images with positions Inspired by convolutional neural networks we address this by adopting notion of locality restricting the positions in the memory matrix to local neighborhood around the query position Changing this neighborhood per query position however would prohibit packing most of the com putation necessary for self attention into two matrix multi plications one for computing the pairwise comparisons and another for generating the weighted averages To avoid this we partition the image into query blocks and associate each of these with larger memory block that also contains the query block For all queries from given query block the model attends to the same memory matrix comprised of all positions from the memory block The self attention is then computed for all query blocks in parallel The feed forward networks and layer normalizations are computed in parallel for all positions In our experiments we use two different schemes for choos ing query blocks and their associated memory block neigh borhoods resulting in two different factorizations of the joint pixel distribution into conditional distributions Both are illustrated in Figure Image Transformer Local Attention For local attention Section we first flatten the input tensor with positional encodings in raster scan order similar to previous work van den Oord et al To compute self attention on the resulting linearized image we then partition the length into non overlapping query blocks of length lq padding with ze roes if necessary While contiguous in the linearized image these blocks can be discontiguous in image coordinate space For each query block we build the memory block from the same positions as and an additional lm positions cor responding to pixels that have been generated before which can result in overlapping memory blocks Local Attention In local attention models we partition the input tensor with positional encodings into rectangular query blocks contiguous in the original image space We generate the image one query block after another ordering the blocks in raster scan order Within each block we generate individual positions or pixel channels again in raster scan order As illustrated in the right half of Figure we generate the blocks outlined in grey lines left to right and top to bottom We use dimensional query blocks of size lq specified by height and width lq wq hq and memory blocks extending the query block to the top left and right by hm wm and again wm pixels respectively In both and local attention we mask attention weights in the query and memory blocks such that posi tions that have not yet been generated are ignored As can be seen in Figure local attention balances hori zontal and vertical conditioning context much more evenly We believe this might have an increasingly positive effect on quality with growing image size as the conditioning informa tion in local attention becomes increasingly dominated by pixels next to given position as opposed to above it Loss Function We perform maximum likelihood in which we maximize log parameters and where the network outputs all parameters of the autoregressive distribution We experiment with two log xt with respect to network settings of the distribution categorical distribution across each channel van den Oord et al and mixture of discretized logistics over three channels Salimans et al The categorical distribution cat captures each intensity value as discrete outcome and factorizes across channels In total there are parameters for each pixel for images the network outputs dimensions Unlike the categorical distribution the discretized mixture of logistics DMOL captures two important properties the ordinal nature of pixel intensities and simpler depen Local Attention Memory Block Memory Block Query Block Local Attention Memory Block Query Block Figure The two different conditional factorizations used in our experiments with and local attention on the left and right respectively In both the image is partitioned into non overlapping query blocks each associated with memory block covering superset of the query block pixels In every self attention layer each position in query block attends to all positions in the memory block The pixel marked as is the last that was generated All channels of pixels in the memory and query blocks shown in white have masked attention weights and do not contribute to the next representations of positions in the query block While the effective receptive field size in this figure is the same for both schemes in attention the memory block contains more evenly balanced number of pixels next to and above the query block respectively dence across channels Salimans et al For each pixel the number of parameters is times the number of mixture components for one unnormalized mixture probability three means three standard deviations and three coefficients which capture the linear dependence For mixtures this translates to parameters for each pixel for im ages the network outputs dimensions which is reduction enabling denser gradients and lower memory Inference Across all of the presented experiments we use categorical sampling during decoding with tempered softmax Dahl et al We adjust the concentration of the distribution we sample from with temperature by which we divide the logits for the channel intensities We tuned between and observing the highest perceptual quality in unconditioned and class conditional image generation with For super resolution we present results for different temperatures in Table Experiments All code we used to develop train and evaluate our models is available in Tensor Tensor Vaswani et al For all experiments we optimize with Adam Kingma Ba and vary the learning rate as specified in Vaswani et al We train our models on both and GPUs with batch sizes ranging from to per GPU Image Transformer Table Bits dim on CIFAR test and ImageNet validation sets The Image Transformer outperforms all models and matches Pixel CNN achieving new state of the art on ImageNet Increasing memory block size bsize significantly improves performance Model Type bsize NLL CIFAR ImageNet Test Validation Pixel CNN Row Pixel RNN Gated Pixel CNN Pixel CNN PixelSNAIL Ours local cat Ours local cat Ours local dmol Table Conditional image generations for all CIFAR cate gories Images on the left are from model that achieves bits dim on the test set Images on the right are from our best non averaged model with bits dim Both models are able to generate convincing cars trucks and ships Generated horses planes and birds also look reasonable Generative Image Modeling Our unconditioned and class conditioned image generation models both use local attention with lq and total memory size of On CIFAR our best unconditional models achieve perplexity of bits dim on the test set using either DMOL or categorical For categorical we use layers with heads feed forward dimension with dropout of In DMOL our best config uses layers heads feed forward dimension and dropout of This is considerable improvement over two baselines the PixelRNN van den Oord et al and PixelCNN Salimans et al Introduced after the Image Transformer the also self attention based Pixel SNAIL model reaches significantly lower perplexity of bits dim on CIFAR Chen et al On the more challenging ImageNet data set however the Image Transformer performs significantly better than PixelSNAIL We also train smaller layer CIFAR models which have dimensions in the feed forward layers attention heads and use dropout of and achieve bits dim matching the PixelCNN model van den Oord et al Our best CIFAR model with DMOL has and feed forward layer layer dimension of and perform attention in dimensions ImageNet is much larger dataset with many more cate gories than CIFAR requiring more parameters in gener ative model Our ImageNet unconditioned generation model has self attention and feed forward layers attention heads dimensions in the feed forward lay ers and dropout of It significantly outperforms the Gated PixelCNN and establishes new state of the art of bits dim with checkpoint averaging We trained only unconditional generative models on ImageNet since class labels were not available in the dataset provided by van den Oord et al Table shows that growing the receptive field improves perplexity significantly We believe this to highlight key advantage of local self attention over CNNs namely that the number of parameters used by local self attention is independent of the size of the receptive field Furthermore while receptivefield self attention still requires fewer floating point operations For experiments with the categorical distribution we evalu ated both coordinate encoding schemes described in Section and found no difference in quality For DMOL we only evaluated learned coordinate embeddings Conditioning on Image Class We represent the image classes as learned dimensional embeddings per class and simply add the respective em bedding to the input representation of every input position together with the positional encodings We trained the class conditioned Image Transformer on CIFAR achieving very similar log likelihoods as in un conditioned generation The perceptual quality of generated Image Transformer images however is significantly higher than that of our unconditioned models The samples from our layer class conditioned models in Table show that we can generate realistic looking images for some categories such as cars and trucks Image Super Resolution Super resolution is the process of recovering high resolu tion image from low resolution image while generating realistic and plausible details Following Dahl et al in our experimental setup we enlarge an pixel image four fold to process that is massively underspec ified the model has to generate aspects such as texture of hair makeup skin and sometimes even gender that cannot possibly be recovered from the source image Here we use the Image Transformer in an encoder decoder configuration connecting the encoder and decoder through an attention mechanism Vaswani et al For the encoder we use embeddings for RGB intensities for each pixel in the image and add dimensional positional encodings for each row and width position Since the input is small we flatten the whole image as tensor where is typically We then feed this sequence to our stack of transformer encoder layers that uses repeated self attention and feed forward layers In the encoder we don require masking but allow any input pixel to attend to any other pixel In the decoder we use stack of local self attention encoder decoder attention and feed forward layers We found using two to three times fewer encoder than decoder layers to be ideal for this task We perform end to end training of the encoder decoder model for Super resolution using the log likelihood ob jective function Our method generates higher resolution images that look plausible and realistic across two datasets For both of the following data sets we resized the image to pixels for the input and pixels for the label using TensorFlow area interpolation method CelebA We trained both our Local and Local mod els on the standard CelebA data set of celebrity faces with cropped boundaries With the Local we achieve nega tive log likelihood NLL of bits dim on the dev set using lq memory size of self attention and feed forward layers attention heads di mensions in the feed forward layers and dropout of With the Local model we only change the query and memory to now represent block of size pixels and pixels respectively This model achieves NLL of bits dim Existing automated metrics like pSNR SSIM and MS SSIM have been shown to not correlate with perceptual image quality Dahl et al Hence we con ducted human evaluation study on Amazon Mechanical Table Negative log likelihood and human eval performance for the Image Transformer on CelebA The fraction of humans fooled is significantly better than the previous state of the art Model Type Fooled ResNet srez GAN PixelRecursive Dahl et al local Image Transformer local Image Transformer Turk where each worker is required to make binary choice when shown one generated and one real image Following the same procedure for the evaluation study as Dahl et al we show pairs of images selected randomly from the validation set to workers each Each generated and original image is upscaled to pixels using the Bilinear interpolation method Each worker then has seconds to make choice between these two images In our method workers choose images from our model up to of the time significant improvement over pre vious models Sampling temperature of and local attention maximized perceptual quality as measured by this evaluation To measure how well the high resolution samples correspond to the low resolution input we calculate Consistency the distance between the low resolution input and bicubic downsampled version of the high resolution sample We observe Consistency score of which is on par with the models in Dahl et al We quantify that our models are more effective than exem plar based Super Resolution techniques like Nearest Neigh bors which perform naive look up of the training data to find the high resolution output We take bicubic down sampled version of our high resolution sample find the nearest low resolution input image in the training data for that sample and calculate the MS SSIM score between the high resolution sample and the corresponding high reso lution image in the training data On average we get MS SSIM score of on samples from the validation set which shows that our models don merely learn to copy training images but generate high quality images by adding synthesized details on the low resolution input image Image Transformer Input Local Attention Local Attention Original Table Images from our and local attention super resolution models trained on CelebA sampled with different temperatures local attention with scored highest in our human evaluation study CIFAR We also trained super resolution model on the CIFAR data set Our model reached negative log likelihood of using local attention and using local attention on the test set As seen in Figure our model commonly generates plausible looking objects even though the input images seem to barely show any discernible structure beyond coarse shapes Conclusion In this work we demonstrate that models based on self attention can operate effectively on modalities other than text and through local self attention scale to significantly larger structures than sentences With fewer layers its larger receptive fields allow the Image Transformer to significantly improve over the state of the art in unconditional probabilis tic image modeling of comparatively complex images from ImageNet as well as super resolution We further hope to have provided additional evidence that even in the light of generative adversarial networks likelihood based models of images is very much promising area for further research as is using network architectures such as the Image Transformer in GANs In future work we would like to explore broader variety of conditioning information including free form text as previously proposed Mansimov et al and tasks combining modalities such as language driven editing of images Fundamentally we aim to move beyond still images to video Kalchbrenner et al and towards applications in model based reinforcement learning References Ba Jimmy Lei Kiros Jamie Ryan and Hinton Geoffrey Layer normalization arXiv preprint arXiv Bellemare Marc Srinivasan Sriram Ostrovski Georg Schaul Tom Saxton David and Munos emi Unifying count based exploration and intrinsic motivation CoRR abs URL http arxiv org abs Bengio Yoshua and Bengio Samy Modeling high dimensional discrete data with multi layer neural net works In Neural Information Processing Systems pp MIT Press Berthelot David Schumm Tom and Metz Luke BEGAN boundary equilibrium generative adversarial networks Image Transformer CoRR abs URL http arxiv org abs Chen Xi Mishra Nikhil Rohaninejad Mostafa and Abbeel Pieter Pixelsnail An improved autoregres sive generative model arXiv preprint arXiv Cheng Jianpeng Dong Li and Lapata Mirella Long short term memory networks for machine reading arXiv preprint arXiv Dahl Ryan Norouzi Mohammad and Shlens Jonathan Pixel recursive super resolution URL https arxiv org abs Goodfellow Ian Pouget Abadie Jean Mirza Mehdi Xu Bing Warde Farley David Ozair Sherjil Courville Aaron and Bengio Yoshua Generative adversarial nets Kalchbrenner Nal and Blunsom Phil Recurrent continuous translation models In Proceedings EMNLP pp URL http nal co papers KalchbrennerBlunsom EMNLP Kalchbrenner Nal van den Oord aron Simonyan Karen Danihelka Ivo Vinyals Oriol Graves Alex and Kavukcuoglu Koray Video pixel networks CoRR abs URL http arxiv org abs Kingma Diederik and Ba Jimmy Adam method for stochastic optimization In ICLR Larochelle Hugo and Murray Iain The neural autoregres sive distribution estimator In The Proceedings of the th International Conference on Artificial Intelligence and Statistics volume of JMLR CP pp Ledig Christian Theis Lucas Huszar Ferenc Caballero Jose Aitken Andrew Tejani Alykhan Totz Johannes Wang Zehan and Shi Wenzhe Photo realistic single image super resolution using generative adversarial net work arXiv Mansimov Elman Parisotto Emilio Ba Lei Jimmy and Salakhutdinov Ruslan Generating images from cap tions with attention CoRR abs URL http arxiv org abs Metz Luke Poole Ben Pfau David and Sohl Dickstein Jascha Unrolled generative adversarial networks CoRR abs URL http arxiv org abs Parikh Ankur ackstr om Oscar Das Dipanjan and Uszkoreit Jakob decomposable attention model In Empirical Methods in Natural Language Process ing URL https arxiv org pdf pdf Radford Alec Metz Luke and Chintala Soumith Unsupervised representation learning with deep con volutional generative adversarial networks CoRR abs URL http arxiv org abs Salimans Tim Karpathy Andrej Chen Xi Kingma Diederik and Bulatov Yaroslav Pixelcnn pix elcnn implementation with discretized logistic mixture likelihood and other modifications In International Con ference on Learning Representations Srivastava Nitish Hinton Geoffrey Krizhevsky Alex Sutskever Ilya and Salakhutdinov Ruslan Dropout simple way to prevent neural networks from overfitting Journal of Machine Learning Research Theis Lucas and Bethge Matthias Generative image mod eling using spatial lstms In Proceedings of the th Inter national Conference on Neural Information Processing Systems Volume NIPS pp Cambridge MA USA MIT Press URL http dl acm org citation cfm id van den Oord aron and Schrauwen Benjamin The student mixture as natural image patch prior with application to image compression Jour nal of Machine Learning Research URL http jmlr org papers vandenoord html van den Oord aron Kalchbrenner Nal and Kavukcuoglu Koray Pixel recurrent neural networks ICML van den Oord aron Kalchbrenner Nal Vinyals Oriol Espeholt Lasse Graves Alex and Kavukcuoglu Koray Conditional image generation with pixelcnn decoders NIPS Vaswani Ashish Shazeer Noam Parmar Niki Uszkoreit Jakob Jones Llion Gomez Aidan Kaiser Lukasz and Polosukhin Illia Attention is all you need URL http arxiv org abs Vaswani Ashish Bengio Samy Brevdo Eugene Chol let Francois Gomez Aidan Gouws Stephan Jones Llion Kaiser ukasz Kalchbrenner Nal Parmar Niki Sepassi Ryan Shazeer Noam and Uszkoreit Jakob Tensor tensor for neural machine translation CoRR abs URL http arxiv org abs Image Transformer Zhang Han Xu Tao Li Hongsheng Zhang Shaoting Huang Xiaolei Wang Xiaogang and Metaxas Dim itris Stackgan Text to photo realistic image synthe sis with stacked generative adversarial networks CoRR abs URL http arxiv org abs 